# 深入理论

> [!IMPORTANT]
>
> 补充：本文参考 [这个项目博文](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E5%85%A5%E9%97%A8)，这里不仅仅是关于分布式，还有一些系统设计的常见知识。这里我读的不是特别细，感觉差不多都学习过，感觉可以直接集成到入门中...

## 1.性能和可拓的关系

如果服务 **性能** 的增长与资源的增加是成比例的，服务就是可扩展的。通常，提高性能意味着服务于更多的工作单元，另一方面，当数据集增长时，同样也可以处理更大的工作单位。

另一个角度来看待性能与可扩展:

- 如果你的系统有 **性能** 问题，对于单个用户来说是缓慢的
- 如果你的系统有 **可扩** 问题，单个用户较快但在高负载下会变慢

上面让我们明确了关系，并且也揭示了区别。

## 2.延迟与吞吐的关系

- **延迟** 是执行操作或运算结果所花费的时间
- **吞吐** 是单位时间内（执行）此类操作或运算的数量

通常，你应该以 **可接受级延迟** 下 **最大化吞吐量** 为目标。

## 3.不同的一致性模式

- **弱一致性**：在写入之后，访问可能看到，也可能看不到（写入数据）。尽力优化之让其能访问最新数据。这种方式可以 `memcached` 等系统中看到。弱一致性在 `VoIP`，视频聊天和实时多人游戏等真实用例中表现不错。打个比方，如果你在通话中丢失信号几秒钟时间，当重新连接时你是听不到这几秒钟所说的话的。
- **最终一致性**：在写入后，访问最终能看到写入数据（通常在数毫秒内）。数据被异步复制。`DNS` 和 `email` 等系统使用的是此种方式。最终一致性在高可用性系统中效果不错。
- **强一致性**：在写入后，访问立即可见。数据被同步复制。文件系统和关系型数据库（`RDBMS`）中使用的是此种方式。强一致性在需要记录的系统中运作良好。

## 4.不同的可用性模式

- **故障切换**：故障切换需要添加额外硬件并增加复杂性，如果新写入数据在能被复制到备用系统之前，工作系统出现了故障，则有可能会丢失数据。
  - **工作到备用切换（Active-passive）**：关于工作到备用的故障切换流程是，工作服务器发送周期信号给待机中的备用服务器。如果周期信号中断，备用服务器切换成工作服务器的 `IP` 地址并恢复服务。宕机时间取决于备用服务器处于“热”待机状态还是需要从“冷”待机状态进行启动。只有工作服务器处理流量。工作到备用的故障切换也被称为主从切换。
  - **双工作切换（Active-active）**：在双工作切换中，双方都在管控流量，在它们之间分散负载。这样哪怕一个节点挂掉仍然有一个节点一直在保持工作（无需交接切换问题，把故障节点的任务全盘接受就可以）。如果是外网服务器，`DNS` 将需要对两方都了解。如果是内网服务器，应用程序逻辑将需要对两方都了解。双工作切换也可以称为主主切换。
- **节点复制**：**主 ─ 从复制** 和 **主 ─ 主复制**

## 5.内容分发网络 CDN

内容分发网络（`CDN`）是一个全球性的代理服务器分布式网络，它从靠近用户的位置提供内容。通常，`HTML/CSS/JS，图片和视频等静态内容` 由 `CDN` 提供，虽然亚马逊 `CloudFront` 等也支持动态内容。`CDN` 的 `DNS` 解析会告知客户端连接哪台服务器。

将内容存储在 `CDN` 上可以从两个方面来提供性能:

- 从靠近用户的数据中心提供资源，更加快速
- 通过 `CDN`，你的服务器不必真的处理请求

`CDN` 有两个重要的动作：

- `CDN` 推送（`push`）：当你服务器上内容发生变动时，推送 `CDN` 接受新内容。直接推送给 `CDN` 并重写 `URL` 地址以指向你的内容的 `CDN` 地址。你可以配置内容到期时间及何时更新。内容只有在更改或新增是才推送，流量最小化，但储存最大化。
- `CDN` 拉取（`pull`）：CDN 拉取模式的关键是只有在第一个用户请求该资源时，才从服务器上拉取资源。你将内容留在自己的服务器上并重写 `URL` 指向 `CDN` 地址。直到内容被缓存在 `CDN` 上为止，对于第一个用户来说，第一次请求只会更慢（但是访问量越来越多的时候就会快很多），存储最小化，但流量最大化。

高流量站点使用 `CDN` 拉取效果不错，因为只有最近请求的内容保存在 `CDN` 中，流量才能更平衡地分散。但是需要注意：

- `CDN` 成本可能因流量而异，可能在权衡之后你将不会使用 `CDN`
- 如果在 `TTL` 过期之前更新内容，`CDN` 缓存内容可能会过时
- `CDN` 需要更改静态内容的 `URL` 地址以指向 `CDN`

## 6.数据库存储

和 `Redis` 集群类似，很多集群的实现方案都是利用主从机制的。主库同时负责读取和写入操作，并复制写入到一个或多个从库中，从库只负责读操作。树状形式的从库再将写入复制到更多的从库中去。如果主库离线，系统可以以只读模式运行，直到某个从库被提升为主库或有新的主库出现。但注意，将从库提升为主库需要额外的逻辑。

也可以选择两个主库都负责读操作和写操作，写入操作时互相协调。如果其中一个主库挂机，系统可以继续读取和写入。主主复制需要添加负载均衡器或者在应用逻辑中做改动，来确定写入哪一个数据库。多数主-主系统要么不能保证一致性（违反 ACID），要么因为同步产生了写入延迟。随着更多写入节点的加入和延迟的提高，如何解决冲突显得越发重要。

或者采用联合的方案（或按功能划分）将数据库按对应功能分割。例如，你可以有三个数据库：**论坛**、**用户** 和 **产品**，而不仅是一个单体数据库，从而减少每个数据库的读取和写入流量，减少复制延迟。较小的数据库意味着更多适合放入内存的数据，进而意味着更高的缓存命中几率。没有只能串行写入的中心化主库，你可以并行写入，提高负载能力。但是这种实现也有很多复杂的地方，如果你的数据库模式需要大量的功能和数据表，联合的效率并不好。你需要更新应用程序的逻辑来确定要读取和写入哪个数据库。用 [server link](http://stackoverflow.com/questions/5145637/querying-data-by-joining-two-tables-in-two-database-on-different-servers) 从两个库联结数据更复杂，联合需要更多的硬件和额外的复杂度。

而分片策略将数据分配在不同的数据库上，使得每个数据库仅管理整个数据集的一个子集。以用户数据库为例，随着用户数量的增加，越来越多的分片会被添加到集群中。

类似联合的优点，分片可以减少读取和写入流量，减少复制并提高缓存命中率。也减少了索引，通常意味着查询更快，性能更好。如果一个分片出问题，其他的仍能运行，你可以使用某种形式的冗余来防止数据丢失。类似联合，没有只能串行写入的中心化主库，你可以并行写入，提高负载能力。常见的做法是用户姓氏的首字母或者用户的地理位置来分隔用户表。但是，你需要修改应用程序的逻辑来实现分片，这会带来复杂的 SQL 查询。分片不合理可能导致数据负载不均衡。例如，被频繁访问的用户数据会导致其所在分片的负载相对其他分片高。再平衡会引入额外的复杂度。基于 [一致性哈希](http://www.paperplanes.de/2011/12/9/the-magic-of-consistent-hashing.html) 的分片算法可以减少这种情况。联结多个分片的数据操作更复杂。分片需要更多的硬件和额外的复杂度。

> [!IMPORTANT]
>
> 补充：`SQL` 调优也是一个经典的主题。
>
> - **基准测试** - 用 [ab](http://httpd.apache.org/docs/2.2/programs/ab.html) 等工具模拟高负载情况。
> - **性能分析** - 通过启用如 [慢查询日志](http://dev.mysql.com/doc/refman/5.7/en/slow-query-log.html) 等工具来辅助追踪性能问题。

## 7.负载均衡器

负载均衡器将传入的请求分发到应用服务器和数据库等计算资源。无论哪种情况，负载均衡器将从计算资源来的响应返回给恰当的客户端。负载均衡器的效用在于:

- 防止请求进入不好的服务器
- 防止资源过载
- 帮助消除单一的故障点
- `SSL 终结`，解密传入的请求并加密服务器响应，这样的话后端服务器就不必再执行这些潜在高消耗运算了。不需要再每台服务器上安装 [X.509 证书](https://en.wikipedia.org/wiki/X.509)。
- `Session 留存`：如果 `Web` 应用程序不追踪会话，发出 `cookie` 并将特定客户端的请求路由到同一实例。

通常会设置采用 [工作 ─ 备用](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#工作到备用切换active-passive) 或 [双工作](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#双工作切换active-active) 模式的多个负载均衡器，以免发生故障。负载均衡器能基于多种方式来路由流量：

- 随机机制：负载均衡器随机选择一台后端服务器处理当前请求，实现简单，无需维护复杂的状态信息。
- 最少负载机制：负载均衡器优先选择当前连接数最少的后端服务器处理请求，需额外开销监控服务器状态，实现稍复杂。
- `Session/cookie` 会话保持机制：确保同一用户的多次请求被转发到同一台后端服务器，以维持会话状态，可能导致部分服务器因绑定用户会话而负载过高。
- [轮询调度或加权轮询调度算法](http://g33kinfo.com/info/archives/2657)：按顺序依次将请求分配给后端服务器（如服务器 A→B→C→A→B→C...），简单公平，适用于所有服务器性能相同的场景。
- [四层负载均衡](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#四层负载均衡)：工作在 OSI 网络模型的 **传输层（第四层）**，基于 `IP` 地址和端口号进行请求转发，转发速度快（仅处理底层协议，无需解析应用层数据），以损失灵活性为代价，四层负载均衡比七层负载均衡花费更少时间和计算资源，虽然这对现代商用硬件的性能影响甚微。
- [七层负载均衡](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#七层负载均衡)：工作在 OSI 网络模型的 **应用层（第七层）**，基于应用层协议内容（如 HTTP/HTTPS）进行请求转发，灵活性高，可实现复杂的路由策略。

如果没有足够的资源配置或配置错误，负载均衡器会变成一个性能瓶颈。引入负载均衡器以帮助消除单点故障但导致了额外的复杂性。单个负载均衡器会导致单点故障，但配置多个负载均衡器会进一步增加复杂性。

## 8.更多级缓存

- **客户端缓存**：缓存可以位于客户端（操作系统或者浏览器），服务端或者不同的缓存层。
- **CDN 缓存**：`CDN` 也被视为一种缓存。
- **Web 服务器缓存**：反向代理和缓存（比如 `Varnish`）可以直接提供静态和动态内容。`Web` 服务器同样也可以缓存请求，返回相应结果而不必连接应用服务器。
- **数据库缓存**：数据库的默认配置中通常包含缓存级别，针对一般用例进行了优化。调整配置，在不同情况下使用不同的模式可以进一步提高性能。
- **应用缓存**：基于内存的缓存比如 `Memcached` 和 `Redis` 是应用程序和数据存储之间的一种键值存储。由于数据保存在 `RAM` 中，它比存储在磁盘上的典型数据库要快多了。`RAM` 比磁盘限制更多，所以例如 `least recently used (LRU)` 的缓存无效算法可以将「热门数据」放在 `RAM` 中，而对一些比较「冷门」的数据不做处理。

> [!IMPORTANT]
> 
>
> 补充：有多个缓存级别，分为两大类：数据库查询和对象。
>
> - 行级别
> - 查询级别
> - 完整的可序列化对象
> - 完全渲染的 `HTML`
>
> 一般来说，你应该尽量避免基于文件的缓存，因为这使得复制和自动缩放很困难。**根据 “计算 / IO 成本” 和 “更新频率” 选择粒度**，也就是说成本越高、更新越慢的内容，越适合缓存。

而何时做缓存就非常有学问了，有几种模式供您参考：

- 缓存模式：第一次查询直接访问数据库，但是查询次数多时同步一部分数据在缓存中，此时先查看缓存中是否有结果再决定是否查询数据库。
- 直写模式：写入到缓存中后立刻返回用户消息，等待后续数据库自动同步数据。
- 回写模式：是直写模式的加强版，在缓存中写入数据后立刻响应用户，然后缓存利用消息队列插入写入任务，利用异步来提高写入性能。
- 刷新模式：是缓存模式的补充，数据库可以主动按照某些策略（比如定时任务），主动刷新数据到缓存中（不过如果不能准确预测到未来需要用到的数据可能会导致性能不如不使用刷新）。

无效缓存是个难题，什么时候更新缓存是与之相关的复杂问题。

## 9.异步型设计

消息队列接收，保留和传递消息。如果按顺序执行操作太慢的话，你可以使用有以下工作流的消息队列：

- 应用程序将作业发布到队列，然后通知用户作业状态
- 一个 `worker` 从队列中取出该作业，对其进行处理，然后显示该作业完成

不去阻塞用户操作，作业在后台处理。在此期间，客户端可能会进行一些处理使得看上去像是任务已经完成了。例如，如果要发送一条推文，推文可能会马上出现在你的时间线上，但是可能需要一些时间才能将你的推文推送到你的所有关注者那里去。

如果队列开始明显增长，那么队列大小可能会超过内存大小，导致高速缓存未命中，磁盘读取，甚至性能更慢。[背压](http://mechanical-sympathy.blogspot.com/2012/05/apply-back-pressure-when-overloaded.html) 可以通过限制队列大小来帮助我们，从而为队列中的作业保持高吞吐率和良好的响应时间。一旦队列填满，客户端将得到服务器忙或者 `HTTP 503` 状态码，以便稍后重试。客户端可以在稍后时间重试该请求，也许是 [指数退避](https://en.wikipedia.org/wiki/Exponential_backoff)。

## 10.CAP 理论

在一个分布式计算系统中，只能同时满足下列的两点:

- **一致性（Consistency）**：每次访问都能获得最新数据但可能会收到错误响应
- **可用性（Availability）**：每次访问都能收到非错响应，但不保证获取到最新数据
- **分区容错性（Partition Tolerance）**：在任意分区网络故障的情况下系统仍能继续运行

网络并不可靠，所以你应要支持分区容错性（这是核心需要），并需要在软件可用性和一致性间做出取舍。因此，**分布式系统必须满足 P**，问题简化为：在 `P` 必然存在的前提下，`C` 和 `A` 能否同时满足？假设一个分布式系统有节点 `A` 和节点 `B`，初始时数据一致（都存储值 `V0`）。此时网络分区发生，`A` 和 `B` 无法通信：

- **若要保证一致性（C）**：当客户端向 `A` 写入新数据 `V1` 后，`A` 需要将 `V1` 同步给 `B` 才能保证所有节点数据一致（满足一致性）。但因分区，`A` 无法同步给 `B`（无法同步而已，但是总系统可以对两者通信）。此时，为了避免 `B` 返回旧数据（`V0`）破坏一致性，系统必须 **拒绝客户端对 B 的读请求**（返回错误或超时），否则 B 会返回不一致的数据。但这就牺牲了 “可用性”（`B` 节点无法响应请求）。
- **若要保证可用性（A）**：当分区发生时，为了让 `A` 和 `B` 都能响应请求（满足可用性），`A` 会接受写入 `V1`，`B` 会继续用旧数据 `V0` 响应读请求。此时客户端从 `A` 读到 `V1`，从 `B` 读到 `V0`，**数据不一致**，即牺牲了 “一致性”。

因此三者是无法同时实现的，最终就会诞生两种分布式架构：

- `CP`：等待分区节点的响应可能会导致延时错误。如果你的业务需求需要原子读写，`CP` 是一个不错的选择。
- `AP`：响应节点上可用数据的最近版本可能并不是最新的。当分区解析完后，写入（操作）可能需要一些时间来传播。如果业务需求允许 [最终一致性](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#最终一致性)，或当有外部故障时要求系统继续运行，`AP` 是一个不错的选择。

## 11.FLP 理论

为了达成一致性时，某些情况下需要让多个工作节点投票达成共识。但是在异步环境中，如果节点间的网络延迟没有上限，只要有一个恶意的节点存在，就没有算法能在有限的时间内达成共识（这是因为无法区分 “消息延迟” 和 “恶意行为”，涉及到拜占庭问题，会让诚实节点无法在有限时间内确定 “其他节点的真实状态”），这就是 `FLP` 理论。

> [!IMPORTANT]
>
> 补充：拜占庭将军问题（`Byzantine Generals Problem`）。这个问题是莱斯利·兰波特（`Leslie Lamport`）于 `1982` 年提出用来解释一致性问题的一个虚构模型（论文地址）。拜占庭是古代东罗马帝国的首都，由于地域宽广，守卫边境的多个将军（系统中的多个节点）需要通过信使来传递消息，达成某些一致的决定。但由于将军中可能存在叛徒（系统中节点出错），这些叛徒将努力向不同的将军发送不同的消息，试图会干扰一致性的达成。拜占庭问题即为在此情况下，如何让忠诚的将军们能达成行动的一致。

解决的常见方法就是 `Las Vegas algorithms`（这个算法又叫撞大运算法，其保证结果正确，只是在运算时所用资源上进行赌博，一个简单的例子是随机快速排序，它的 `pivot, 基准值` 是随机选的，但排序结果永远一致，也就是那句“在绝对随机情况下，次数越多，成功概率越高”）在每一轮皆有一定机率达成共识，随着时间增加，机率会越趋近于 `1`。而这也是许多成功的共识算法会采用的解决问题的办法。

- （下达可信的规定区间）在每轮共识中，节点可以 **随机选择一个等待时间窗口**（而非固定时长）。恶意节点即使试图通过延迟消息干扰，也无法精准预测诚实节点的等待窗口 —— 随着轮次增加，诚实节点恰好避开干扰、收集到足够多正确信息的概率会越来越高。
- （信任更加信任的节点）再如，节点可以 **随机选择提议值**（而非固定逻辑生成）。即使部分节点故障，随机选择能让诚实节点的提议在统计上更可能形成多数，从而在某一轮中意外达成共识。

简单说，随机性让恶意节点的 “针对性干扰” 失效，而 “多次尝试” 则通过概率累积，最终让诚实节点突破 “不可区分性” 的僵局，达成正确共识。

## 12.DLS 理论

容错的上限，[从 DLS 论文 中](https://groups.csail.mit.edu/tds/papers/Lynch/jacm88.pdf) 我们可以得到以下结论：在部分同步（`partially synchronous`）的网络环境中（即网络延迟有一定的上限，但我们无法事先知道上限是多少），协议可以容忍最多 `1/3` 的拜占庭故障（`Byzantine fault`）。

> [!IMPORTANT]
>
> 补充：拜占庭故障（`Byzantine Fault`）是分布式系统中最严重的一类节点故障，指节点（如服务器、进程等）出现 **任意行为** 的故障，包括恶意欺骗、篡改消息、故意发送错误信息、不响应或伪造数据等，甚至可以合谋干扰系统共识。

- 在异步（`asynchronous`）的网络环境中，具有确定性质的协议无法容忍任何错误，但这篇论文并没有提及 `randomized algorithms`，在这种情况下可以容忍最多 `1/3` 的拜占庭故障。
- 在同步（synchronous）网络环境中（即网络延迟有上限且上限是已知的），协议可以容忍 `100%` 的拜占庭故障，但当超过 `1/2` 的节点为恶意节点时，会有一些限制条件。要注意的是，我们考虑的是 "具有认证特性的拜占庭模型（`authenticated Byzantine`）“，而不是" 一般的拜占庭模型”（具有认证特性指的是将如今已经过大量研究且成本低廉的公私钥加密机制应用在我们的算法中）。

## 13.荒谬的 8 假设

- 网络是稳定的
- 网络传输的延迟是零
- 网络的带宽是无穷大
- 网络是安全的
- 网络的拓扑不会改变
- 只有一个系统管理员
- 传输数据的成本为零
- 整个网络是同构的（同构是指“节点之间结构类似”）

为什么需要理解这 `8` 个错误？[看这篇](http://www.rgoarchitects.com/Files/fallacies.pdf) 就可以得知。

## 14.其他补充

一些图书 https://time.geekbang.org/column/article/10604

实践过程 https://time.geekbang.org/column/article/11232

真实架构

[![img](./assets/TcUo2fw.png)](https://github.com/donnemartin/system-design-primer/blob/master/images/TcUo2fw.png)
**[Source: Twitter timelines at scale](https://www.infoq.com/presentations/Twitter-Timeline-Scalability)**

**不要专注于以下文章的细节，专注于以下方面：**

- 发现这些文章中的共同的原则、技术和模式。
- 学习每个组件解决哪些问题，什么情况下使用，什么情况下不适用
- 复习学过的文章

| 类型            | 系统                                                   | 引用                                                         |
| --------------- | ------------------------------------------------------ | ------------------------------------------------------------ |
| Data processing | **MapReduce** - Google 的分布式数据处理                 | [research.google.com](http://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/mapreduce-osdi04.pdf) |
| Data processing | **Spark** - Databricks 的分布式数据处理                | [slideshare.net](http://www.slideshare.net/AGrishchenko/apache-spark-architecture) |
| Data processing | **Storm** - Twitter 的分布式数据处理                   | [slideshare.net](http://www.slideshare.net/previa/storm-16094009) |
|                 |                                                        |                                                              |
| Data store      | **Bigtable** - Google 的列式数据库                     | [harvard.edu](http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/chang06bigtable.pdf) |
| Data store      | **HBase** - Bigtable 的开源实现                        | [slideshare.net](http://www.slideshare.net/alexbaranau/intro-to-hbase) |
| Data store      | **Cassandra** - Facebook 的列式数据库                  | [slideshare.net](http://www.slideshare.net/planetcassandra/cassandra-introduction-features-30103666) |
| Data store      | **DynamoDB** - Amazon 的文档数据库                     | [harvard.edu](http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/decandia07dynamo.pdf) |
| Data store      | **MongoDB** - 文档数据库                               | [slideshare.net](http://www.slideshare.net/mdirolf/introduction-to-mongodb) |
| Data store      | **Spanner** - Google 的全球分布数据库                  | [research.google.com](http://research.google.com/archive/spanner-osdi2012.pdf) |
| Data store      | **Memcached** - 分布式内存缓存系统                     | [slideshare.net](http://www.slideshare.net/oemebamo/introduction-to-memcached) |
| Data store      | **Redis** - 能够持久化及具有值类型的分布式内存缓存系统 | [slideshare.net](http://www.slideshare.net/dvirsky/introduction-to-redis) |
|                 |                                                        |                                                              |
| File system     | **Google File System (GFS)** - 分布式文件系统          | [research.google.com](http://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/gfs-sosp2003.pdf) |
| File system     | **Hadoop File System (HDFS)** - GFS 的开源实现         | [apache.org](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html) |
|                 |                                                        |                                                              |
| Misc            | **Chubby** - Google 的分布式系统的低耦合锁服务         | [research.google.com](http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/chubby-osdi06.pdf) |
| Misc            | **Dapper** - 分布式系统跟踪基础设施                    | [research.google.com](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36356.pdf) |
| Misc            | **Kafka** - LinkedIn 的发布订阅消息系统                | [slideshare.net](http://www.slideshare.net/mumrah/kafka-talk-tri-hug) |
| Misc            | **Zookeeper** - 集中的基础架构和协调服务               | [slideshare.net](http://www.slideshare.net/sauravhaloi/introduction-to-apache-zookeeper) |
|                 | 添加更多                                               | [贡献](https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md#贡献) |

### 公司的系统架构

| Company        | Reference(s)                                                 |
| -------------- | ------------------------------------------------------------ |
| Amazon         | [Amazon 的架构](http://highscalability.com/amazon-architecture) |
| Cinchcast      | [每天产生 1500 小时的音频](http://highscalability.com/blog/2012/7/16/cinchcast-architecture-producing-1500-hours-of-audio-every-d.html) |
| DataSift       | [每秒实时挖掘 120000 条 tweet](http://highscalability.com/blog/2011/11/29/datasift-architecture-realtime-datamining-at-120000-tweets-p.html) |
| DropBox        | [我们如何缩放 Dropbox](https://www.youtube.com/watch?v=PE4gwstWhmc) |
| ESPN           | [每秒操作 100000 次](http://highscalability.com/blog/2013/11/4/espns-architecture-at-scale-operating-at-100000-duh-nuh-nuhs.html) |
| Google         | [Google 的架构](http://highscalability.com/google-architecture) |
| Instagram      | [1400 万用户，达到兆级别的照片存储](http://highscalability.com/blog/2011/12/6/instagram-architecture-14-million-users-terabytes-of-photos.html) [是什么在驱动 Instagram](http://instagram-engineering.tumblr.com/post/13649370142/what-powers-instagram-hundreds-of-instances) |
| Justin.tv      | [Justin.Tv 的直播广播架构](http://highscalability.com/blog/2010/3/16/justintvs-live-video-broadcasting-architecture.html) |
| Facebook       | [Facebook 的可扩展 memcached](https://cs.uwaterloo.ca/~brecht/courses/854-Emerging-2014/readings/key-value/fb-memcached-nsdi-2013.pdf) [TAO: Facebook 社交图的分布式数据存储](https://cs.uwaterloo.ca/~brecht/courses/854-Emerging-2014/readings/data-store/tao-facebook-distributed-datastore-atc-2013.pdf) [Facebook 的图片存储](https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Beaver.pdf) |
| Flickr         | [Flickr 的架构](http://highscalability.com/flickr-architecture) |
| Mailbox        | [在 6 周内从 0 到 100 万用户](http://highscalability.com/blog/2013/6/18/scaling-mailbox-from-0-to-one-million-users-in-6-weeks-and-1.html) |
| Pinterest      | [从零到每月数十亿的浏览量](http://highscalability.com/blog/2013/4/15/scaling-pinterest-from-0-to-10s-of-billions-of-page-views-a.html) [1800 万访问用户，10 倍增长，12 名员工](http://highscalability.com/blog/2012/5/21/pinterest-architecture-update-18-million-visitors-10x-growth.html) |
| Playfish       | [月用户量 5000 万并在不断增长](http://highscalability.com/blog/2010/9/21/playfishs-social-gaming-architecture-50-million-monthly-user.html) |
| PlentyOfFish   | [PlentyOfFish 的架构](http://highscalability.com/plentyoffish-architecture) |
| Salesforce     | [他们每天如何处理 13 亿笔交易](http://highscalability.com/blog/2013/9/23/salesforce-architecture-how-they-handle-13-billion-transacti.html) |
| Stack Overflow | [Stack Overflow 的架构](http://highscalability.com/blog/2009/8/5/stack-overflow-architecture.html) |
| TripAdvisor    | [40M 访问者，200M 页面浏览量，30TB 数据](http://highscalability.com/blog/2011/6/27/tripadvisor-architecture-40m-visitors-200m-dynamic-page-view.html) |
| Tumblr         | [每月 150 亿的浏览量](http://highscalability.com/blog/2012/2/13/tumblr-architecture-15-billion-page-views-a-month-and-harder.html) |
| Twitter        | [Making Twitter 10000 percent faster](http://highscalability.com/scaling-twitter-making-twitter-10000-percent-faster) [每天使用 MySQL 存储 2.5 亿条 tweet](http://highscalability.com/blog/2011/12/19/how-twitter-stores-250-million-tweets-a-day-using-mysql.html) [150M 活跃用户，300K QPS，22 MB/S 的防火墙](http://highscalability.com/blog/2013/7/8/the-architecture-twitter-uses-to-deal-with-150m-active-users.html) [可扩展时间表](https://www.infoq.com/presentations/Twitter-Timeline-Scalability) [Twitter 的大小数据](https://www.youtube.com/watch?v=5cKTP36HVgI) [Twitter 的行为：规模超过 1 亿用户](https://www.youtube.com/watch?v=z8LU0Cj6BOU) |
| Uber           | [Uber 如何扩展自己的实时化市场](http://highscalability.com/blog/2015/9/14/how-uber-scales-their-real-time-market-platform.html) |
| WhatsApp       | [Facebook 用 190 亿美元购买 WhatsApp 的架构](http://highscalability.com/blog/2014/2/26/the-whatsapp-architecture-facebook-bought-for-19-billion.html) |
| YouTube        | [YouTube 的可扩展性](https://www.youtube.com/watch?v=w5WVu624fY8) [YouTube 的架构](http://highscalability.com/youtube-architecture) |

### 公司工程博客



> 你即将面试的公司的架构
>
> 你面对的问题可能就来自于同样领域

- [Airbnb Engineering](http://nerds.airbnb.com/)
- [Atlassian Developers](https://developer.atlassian.com/blog/)
- [Autodesk Engineering](http://cloudengineering.autodesk.com/blog/)
- [AWS Blog](https://aws.amazon.com/blogs/aws/)
- [Bitly Engineering Blog](http://word.bitly.com/)
- [Box Blogs](https://www.box.com/blog/engineering/)
- [Cloudera Developer Blog](http://blog.cloudera.com/blog/)
- [Dropbox Tech Blog](https://tech.dropbox.com/)
- [Engineering at Quora](http://engineering.quora.com/)
- [Ebay Tech Blog](http://www.ebaytechblog.com/)
- [Evernote Tech Blog](https://blog.evernote.com/tech/)
- [Etsy Code as Craft](http://codeascraft.com/)
- [Facebook Engineering](https://www.facebook.com/Engineering)
- [Flickr Code](http://code.flickr.net/)
- [Foursquare Engineering Blog](http://engineering.foursquare.com/)
- [GitHub Engineering Blog](https://github.blog/category/engineering)
- [Google Research Blog](http://googleresearch.blogspot.com/)
- [Groupon Engineering Blog](https://engineering.groupon.com/)
- [Heroku Engineering Blog](https://engineering.heroku.com/)
- [Hubspot Engineering Blog](http://product.hubspot.com/blog/topic/engineering)
- [High Scalability](http://highscalability.com/)
- [Instagram Engineering](http://instagram-engineering.tumblr.com/)
- [Intel Software Blog](https://software.intel.com/en-us/blogs/)
- [Jane Street Tech Blog](https://blogs.janestreet.com/category/ocaml/)
- [LinkedIn Engineering](http://engineering.linkedin.com/blog)
- [Microsoft Engineering](https://engineering.microsoft.com/)
- [Microsoft Python Engineering](https://blogs.msdn.microsoft.com/pythonengineering/)
- [Netflix Tech Blog](http://techblog.netflix.com/)
- [Paypal Developer Blog](https://devblog.paypal.com/category/engineering/)
- [Pinterest Engineering Blog](http://engineering.pinterest.com/)
- [Quora Engineering](https://engineering.quora.com/)
- [Reddit Blog](http://www.redditblog.com/)
- [Salesforce Engineering Blog](https://developer.salesforce.com/blogs/engineering/)
- [Slack Engineering Blog](https://slack.engineering/)
- [Spotify Labs](https://labs.spotify.com/)
- [Stripe Engineering Blog](https://stripe.com/blog/engineering)
- [Twilio Engineering Blog](http://www.twilio.com/engineering)
- [Twitter Engineering](https://engineering.twitter.com/)
- [Uber Engineering Blog](http://eng.uber.com/)
- [Yahoo Engineering Blog](http://yahooeng.tumblr.com/)
- [Yelp Engineering Blog](http://engineeringblog.yelp.com/)
- [Zynga Engineering Blog](https://www.zynga.com/blogs/engineering)

#### 来源及延伸阅读

- [kilimchoi/engineering-blogs](https://github.com/kilimchoi/engineering-blogs)

